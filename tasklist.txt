Sungevity Interview 20201113
-------------------

+ Talk to public api of hackernews
    + figure out which endpoints to use
    + make initial calls to check the data structure

- Readme.MD

- Comments 

+ refactoring

+ Remove unnecessary logs

- install and run instructions

- deploy?

- throttling?

- caching options?

- tests
    - structure



+ Setup server
    + Node 12.19
    + nodemon
    + koa
    + return initial data from /recent endpoint

- Setup endpoints
    + /recent
    + /historical
    - /karma

- /recent
    // Recent top words: return a set of 25 words that are the most occurring words in the titles of the last 250 posted stories. 
    // The response should include the number of times each word is occurring.

    + Call hackernews api to get last 250 story titles
        -> https://hacker-news.firebaseio.com/v0/newstories.json
        -> Limit top 250
    -> For every id in list
        + Get Title
        + Sanitize title with stop words
        + Count Words
        + Save for later use
            + in memory or file
                + go with in memory for now, that can be persisted to a file later
    + Get words from each title
        + Create a global dictionary for all the words
    + Create a dictionary
    + Aggregate the results
    + improve sanitation

- /historical
    -* parallelize the calls
    + Get the lastest 1000 items
        - Walk back from last one
    + filter out the ones that are not comments.
    + Nicely parallelize the calls with 100 max concurrency level
        + Clean code
        + Speedup resolveReponses
    + don't send a new request if item is in cache
        + only send a request for newer items
    + Sanitize the "text" field.
    + Process for aggregation

- [DO LAST] /karma 
    - Figure this out
        - What exactly is karma
        - How the calculation works

